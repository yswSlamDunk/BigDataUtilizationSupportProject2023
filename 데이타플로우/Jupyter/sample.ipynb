{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8fa6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clawringContentImage(href):\n",
    "    response = requests.get(href)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        res = soup.find('h3', {'id' : 'view_content'})\n",
    "        content = res.text\n",
    "        images = res.find_all('img', {'class' : 'lazy content-image'})\n",
    "\n",
    "        if images != []:\n",
    "            imageList = [url['data-original'] for url in res.find_all('img', {'class' : 'lazy content-image'})]\n",
    "        else:\n",
    "            imageList = []\n",
    "        return content, imageList\n",
    "    \n",
    "    else:\n",
    "        return np.nan, []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ca5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_save(re):\n",
    "#     re, save_path, fieldNames = zip_data\n",
    "#     print(save_path, fieldNames)\n",
    "    \n",
    "    re = BeautifulSoup(re, 'html.parser')\n",
    "    category = re.find('a', {'class' : 'mw_basic_list_category'}).text\n",
    "    date = re.find('div', {'class' : 'mw_basic_list_datetime media-no-text'}).text\n",
    "    view = re.find('div', {'class' : 'mw_basic_list_hit media-no-text'}).text\n",
    "    title = re.find('h3', {'class' : 'media-list-subject flex'}).text.replace('\\n', '')\n",
    "    rank = str(re.find('img')['src']).split('/')[-1].split('.')[0]\n",
    "    href = re.find_all('a')[1]['href']\n",
    "    try:\n",
    "        writer = re.find('a', {'class' : 'sv_member'}).text.strip()\n",
    "    except:\n",
    "        writer = re.find('div', {'mw_basic_list_name media-no-text'}).text\n",
    "    content, imageList = clawringContentImage(href)\n",
    "\n",
    "    with open(save_path, 'a', newline = '') as csvfile:\n",
    "        csvWriter = csv.DictWriter(csvfile, fieldnames = fieldNames)\n",
    "        \n",
    "        try:\n",
    "            csvWriter.writerow({'title' : title,\n",
    "                                'category' : category,\n",
    "                                'date' : date,\n",
    "                                'writer' : writer,\n",
    "                                'rank' : rank,\n",
    "                                'view' : view,\n",
    "                                'href' : href,\n",
    "                                'content' : content,\n",
    "                                'imageList' : imageList})      \n",
    "        except:\n",
    "            print(href, ': Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aecde63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clawringHref01(url, pageNum, startPage = 1):\n",
    "    errorNum = 0\n",
    "#     path_save_base = '../Data'\n",
    "#     save_path = os.path.join(path_save_base, path_save)\n",
    "    if not os.path.isfile(save_path):\n",
    "        tmp = pd.DataFrame(columns = fieldNames)\n",
    "        tmp.to_csv(save_path, index = False)\n",
    "        \n",
    "        with open(save_path, 'w', newline = '') as csvfile:\n",
    "            csvWriter = csv.DictWriter(csvfile, fieldnames = fieldNames)\n",
    "            csvWriter.writeheader()\n",
    "\n",
    "    for page in tqdm(range(startPage, pageNum + 1)):\n",
    "        page = str(page)\n",
    "        response = requests.get(url + page)\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            re_list = soup.find_all('div', {\"class\":\"list_wrap flex_spbtw\"})\n",
    "            \n",
    "            re_list = [str(re) for re in re_list]\n",
    "\n",
    "#             for i in zip(re_list, [save_path] * len(re_list), [fieldNames] * len(re_list)):\n",
    "#                 parsing_save(i)\n",
    "            print(multiprocessing.cpu_count())\n",
    "    \n",
    "            with multiprocessing.Pool(multiprocessing.cpu_count() -2) as p:\n",
    "                p.map(parsing_save, re_list)\n",
    "\n",
    "#             with multiprocessing.Pool(multiprocessing.cpu_count() -2) as p:\n",
    "#                 print('1234')\n",
    "#                 p.starmap(parsing_save, [(x, save_path, fieldNames) for x in re_list])\n",
    "#                 func = partial(parsing_save, )\n",
    "#                 p.map(parsing_save, (re_list, [save_path] * len(re_list), [fieldNames] * len(re_list)))\n",
    "#                 p.map(parsing_save, zip(re_list, [save_path] * len(re_list), [fieldNames] * len(re_list)))\n",
    "\n",
    "#             p = multiprocessing.Process(target = parsing_save, args = zip(re_list, [save_path] * len(re_list), [fieldNames] * len(re_list)))\n",
    "#             p.start()\n",
    "#             p.join()\n",
    "\n",
    "#             for re in re_list:\n",
    "#                 parsing_save(re, save_path, fieldNames)\n",
    "#                 parsing_save(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ecf640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigDataUtilization2023",
   "language": "python",
   "name": "bigdatautilization2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
