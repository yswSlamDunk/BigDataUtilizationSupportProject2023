{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1fd481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0abaee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clawringContentImage(href):\n",
    "    response = requests.get(href)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        res = soup.find('h3', {'id' : 'view_content'})\n",
    "        content = res.text\n",
    "        images = res.find_all('img', {'class' : 'lazy content-image'})\n",
    "\n",
    "        if images != []:\n",
    "            imageList = [url['data-original'] for url in res.find_all('img', {'class' : 'lazy content-image'})]\n",
    "        else:\n",
    "            imageList = []\n",
    "        return content, imageList\n",
    "    \n",
    "    else:\n",
    "        return np.nan, []    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d239ee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_save(re):\n",
    "    re = BeautifulSoup(re, 'html.parser')\n",
    "    category = re.find('a', {'class' : 'mw_basic_list_category'}).text\n",
    "    date = re.find('div', {'class' : 'mw_basic_list_datetime media-no-text'}).text\n",
    "    view = re.find('div', {'class' : 'mw_basic_list_hit media-no-text'}).text\n",
    "    title = re.find('h3', {'class' : 'media-list-subject flex'}).text.replace('\\n', '')\n",
    "    rank = str(re.find('img')['src']).split('/')[-1].split('.')[0]\n",
    "    href = re.find_all('a')[1]['href']\n",
    "    try:\n",
    "        writer = re.find('a', {'class' : 'sv_member'}).text.strip()\n",
    "    except:\n",
    "        writer = re.find('div', {'mw_basic_list_name media-no-text'}).text\n",
    "    content, imageList = clawringContentImage(href)\n",
    "\n",
    "    with open(save_path, 'a', newline = '') as csvfile:\n",
    "        csvWriter = csv.DictWriter(csvfile, fieldnames = fieldNames)\n",
    "        \n",
    "        try:\n",
    "            csvWriter.writerow({'title' : title,\n",
    "                                'category' : category,\n",
    "                                'date' : date,\n",
    "                                'writer' : writer,\n",
    "                                'rank' : rank,\n",
    "                                'view' : view,\n",
    "                                'href' : href,\n",
    "                                'content' : content,\n",
    "                                'imageList' : imageList})      \n",
    "        except:\n",
    "            print(href, ': Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "111f83ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clawringHref01(url, pageNum, startPage = 1):\n",
    "    errorNum = 0\n",
    "\n",
    "    if not os.path.isfile(save_path):\n",
    "        tmp = pd.DataFrame(columns = fieldNames)\n",
    "        tmp.to_csv(save_path, index = False)\n",
    "        \n",
    "        with open(save_path, 'w', newline = '') as csvfile:\n",
    "            csvWriter = csv.DictWriter(csvfile, fieldnames = fieldNames)\n",
    "            csvWriter.writeheader()\n",
    "\n",
    "    for page in tqdm(range(startPage, pageNum + 1)):\n",
    "        page = str(page)\n",
    "        response = requests.get(url + page)\n",
    "        if response.status_code == 200:\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            re_list = soup.find_all('div', {\"class\":\"list_wrap flex_spbtw\"})            \n",
    "            re_list = [str(re) for re in re_list]\n",
    "\n",
    "            for re in re_list:\n",
    "                parsing_save(re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee5cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e649b9b4",
   "metadata": {},
   "source": [
    "### 대다모 크롤링 대상\n",
    "* 샴푸&두피케어\n",
    "    * 샴푸&두피제품 찾기\n",
    "    * 체험평가단\n",
    "* 탈모톡톡\n",
    "    * 탈모수다 : (완)\n",
    "    * 샴푸&영양제 : (완)\n",
    "* 탈모후기\n",
    "    * 탈모치료성공스토리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d734b983",
   "metadata": {},
   "source": [
    "1. 머리말 : category\n",
    "2. 제목 : title\n",
    "3. 작성자 : writer\n",
    "4. 작성일 : date\n",
    "5. 조회수 : view\n",
    "6. 링크 : href\n",
    "7. 계급 : rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee38c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = 'href_탈모톡톡_탈모수다.csv'\n",
    "path_save_base = '../Data'\n",
    "save_path = os.path.join(path_save_base, path_save)\n",
    "fieldNames = ['title', 'category', 'date', 'writer', 'rank', 'view', 'href', 'content', 'imageList']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbd6a16",
   "metadata": {},
   "source": [
    "#### 탈모톡톡_탈모수다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d99b889",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://daedamo.com/story?overlaps=1&page='\n",
    "path_save = 'href_탈모톡톡_탈모수다.csv'\n",
    "fieldNames = ['title', 'category', 'date', 'writer', 'rank', 'view', 'href', 'content', 'imageList']\n",
    "pageNum = 2\n",
    "startPage = 1\n",
    "clawringHref01(url, pageNum, startPage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553f04b9",
   "metadata": {},
   "source": [
    "#### 탈모톡톡_샴푸&영양제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0512466",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://daedamo.com/balmo?overlaps=1&page='\n",
    "path_save = 'href_탈모톡톡_샴푸&영양제.csv'\n",
    "fieldNames = ['title', 'category', 'date', 'writer', 'rank', 'view', 'href', 'content', 'imageList']\n",
    "pageNum = 120\n",
    "\n",
    "clawringHref01(url, pageNum, path_save, fieldNames, startPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e1e233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeabade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91239d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e6522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6372f06b",
   "metadata": {},
   "source": [
    "# 샴푸&두피케어-샴푸&두피케어\n",
    "이거 같은 경우에는, 로그인을 해야하며, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d8470b",
   "metadata": {},
   "source": [
    "### selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5c88136",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_url = 'https://daedamo.com/new/'\n",
    "login_url = 'https://daedamo.com/new/bbs/login.php'\n",
    "\n",
    "shamFind_url = 'https://daedamo.com/ingre?sca=%ED%83%88%EB%AA%A8%EA%B4%80%EB%A0%A8%EC%83%81%ED%92%88&bso=A&overlaps=5'\n",
    "startPage = 1\n",
    "endPage = 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d36d3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawReview(soup):\n",
    "    commentList = soup.find_all('div', {'class', 'comment-item'})\n",
    "    \n",
    "    totalScoreList = []\n",
    "    satisfactionScoreList = []\n",
    "    priceScoreList = []\n",
    "    rebuyScoreList = []\n",
    "    commenterList = []\n",
    "    commentDateList = []\n",
    "    commentGoodList = []\n",
    "    commentBadList = []\n",
    "    commentContentList = []\n",
    "    \n",
    "    for comment in commentList:\n",
    "        reviewScoreList = comment.find_all('div', {'class', 'review-item'})\n",
    "\n",
    "        commenter = comment.find('span', {'class' : 'sv_wrap'})\n",
    "        if pd.isnull(commenter):\n",
    "            commenter = comment.find('span', {'class' : 'guest'}).text\n",
    "        else:\n",
    "            commenter = comment.find('a')['title'].replace(' 자기소개', '')\n",
    "        \n",
    "        commentDate = comment.find('span', {'class' : 'datetime'}).text  \n",
    "        commentContent = comment.find('div', {'class' : 'contents'})\n",
    "        \n",
    "        \n",
    "        commenterList.append(commenter)\n",
    "        commentDateList.append(commentDate)\n",
    "        \n",
    "        \n",
    "        if len(reviewScoreList) == 1:\n",
    "            satisfactionScore = np.nan\n",
    "            priceScore = np.nan\n",
    "            rebuyScore = np.nan\n",
    "            totalScore = reviewScoreList[0].find('span')['style'].replace('width:', '')[:-1]\n",
    "            good = np.nan\n",
    "            bad = np.nan\n",
    "            commentContent = commentContent.text\n",
    "                \n",
    "        else:\n",
    "            satisfactionScore = reviewScoreList[0].find('span')['style'].replace('width:', '')[:-1]\n",
    "            priceScore = reviewScoreList[1].find('span')['style'].replace('width:', '')[:-1]\n",
    "            rebuyScore = reviewScoreList[2].find('span')['style'].replace('width:', '')[:-1]\n",
    "            totalScore = np.nan\n",
    "            \n",
    "            tmp_content = commentContent.find_all('dt')\n",
    "            \n",
    "            if len(tmp_content) == 2:\n",
    "                commentContent = commentContent.text.split('아쉬운점')\n",
    "                good = commentContent[0].split('좋은점')[1]\n",
    "                bad = commentContent[1]\n",
    "                \n",
    "            else:\n",
    "                good = commentContent.text.split('좋은점')[1]\n",
    "                bad = np.nan\n",
    "            commentContent = np.nan\n",
    "            \n",
    "        commentGoodList.append(good)\n",
    "        commentBadList.append(bad)\n",
    "        commentContentList.append(commentContent)      \n",
    "        \n",
    "        satisfactionScoreList.append(satisfactionScore)\n",
    "        priceScoreList.append(priceScore)\n",
    "        rebuyScoreList.append(rebuyScore)\n",
    "        totalScoreList.append(totalScore)\n",
    "\n",
    "    return [totalScoreList, satisfactionScoreList, priceScoreList, rebuyScoreList, commenterList, commentDateList, commentContentList, commentGoodList, commentBadList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd538a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResult(result1, resut2, savePath = '../Data/샴푸&두피제품 찾기.csv', fieldNames = ['href', 'title', 'reviewNum', 'tag', 'brand', 'company', 'howToUse', 'ingredients', 'image', 'volume', 'price', 'totalScore', 'satisfactionScore', 'priceScore', 'rebuyScore', 'commenter', 'commentDate', 'commentContent', 'commentGood', 'commentBad']):\n",
    "        if result2 == []:\n",
    "            with open(savePath, 'a', newline = '', encoding = 'utf-8-sig') as csvfile:\n",
    "                csvWriter = csv.DictWriter(csvfile, fieldnames = fieldNames)\n",
    "                csvWriter.writerow({'href' : result1[0],\n",
    "                                    'title' : result1[1],\n",
    "                                    'reviewNum' : result1[2],\n",
    "                                    'tag' : result1[3],\n",
    "                                    'brand' : result1[4],\n",
    "                                    'company' : result1[5],\n",
    "                                    'howToUse' : result1[6],\n",
    "                                    'ingredients' : result1[7],\n",
    "                                    'image' : result1[8],\n",
    "                                    'volume' : result1[9],\n",
    "                                    'price' : result1[10],\n",
    "                                    'totalScore' : np.nan,\n",
    "                                    'satisfactionScore' : np.nan,\n",
    "                                    'priceScore' : np.nan,\n",
    "                                    'rebuyScore' : np.nan,\n",
    "                                    'commenter' : np.nan,\n",
    "                                    'commentDate' : np.nan,\n",
    "                                    'commentContent' : np.nan,\n",
    "                                    'commentGood' : np.nan,\n",
    "                                    'commentBad' : np.nan\n",
    "                                   })\n",
    "        else:\n",
    "            tmp = [np.nan]*(len(result2[0]) -1)\n",
    "            href = [result1[0]] + tmp\n",
    "            title = [result1[1]] + tmp\n",
    "            reviewNum = [result1[2]] + tmp\n",
    "            tag = [result1[3]] + tmp\n",
    "            brand = [result1[4]] + tmp\n",
    "            company = [result1[5]] + tmp\n",
    "            howToUse = [result1[6]] + tmp\n",
    "            ingredients = [result1[7]] + tmp\n",
    "            image = [result1[8]] + tmp\n",
    "            volume = [result1[9]] + tmp\n",
    "            price = [result1[10]] + tmp\n",
    "            pd.DataFrame({'href' : href,\n",
    "                          'title' : title,\n",
    "                          'reviewNum' : reviewNum,\n",
    "                          'tag' : tag,\n",
    "                          'brand' : brand,\n",
    "                          'company' : company,\n",
    "                          'howToUse' : howToUse,\n",
    "                          'ingredients' : ingredients,\n",
    "                          'image' : image,\n",
    "                          'volume' : volume,\n",
    "                          'price' : price,\n",
    "                          'totalScore' : result2[0],\n",
    "                          'satisfactionScore' : result2[1],\n",
    "                          'priceScore' : result2[2],\n",
    "                          'rebuyScore' : result2[3],\n",
    "                          'commenter' : result2[4],\n",
    "                          'commentDate' : result2[5],\n",
    "                          'commentContent' : result2[6],\n",
    "                          'commentGood' : result2[7],\n",
    "                          'commentBad' : result2[8],\n",
    "                         }).to_csv(savePath, mode = 'a', index = False, header = None)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87273d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82613ad0dbd477ab53b191c4396d01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disabled-dev-shm-usage')\n",
    "chrome_options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "chrome_options.add_argument('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36')\n",
    "\n",
    "savePath = '../Data/샴푸&두피제품 찾기.csv'\n",
    "fieldNames = ['href', 'title', 'reviewNum', 'tag', 'brand', 'company', 'howToUse', 'ingredients', \\\n",
    "             'image', 'volume', 'price', 'totalScore', 'satisfactionScore', 'priceScore', \\\n",
    "             'rebuyScore', 'commenter', 'commentDate', 'commentContent', 'commentGood', 'commentBad']\n",
    "\n",
    "driver = webdriver.Chrome(options = chrome_options)\n",
    "\n",
    "driver.get(login_url)\n",
    "driver.implicitly_wait(np.random.rand(1)[0])\n",
    "\n",
    "driver.find_element_by_id('login_id').send_keys('dataflow28')\n",
    "driver.find_element_by_id('login_pw').send_keys('flowdata2833!!')\n",
    "driver.find_element_by_class_name('btn_submit').click()\n",
    "driver.implicitly_wait(np.random.rand(1)[0])\n",
    "time.sleep(2)\n",
    "\n",
    "numCompile = re.compile('\\d')\n",
    "\n",
    "if not os.path.isfile(savePath):\n",
    "    tmp = pd.DataFrame(columns = fieldNames)\n",
    "    tmp.to_csv(savePath, encoding = 'utf-8-sig')\n",
    "    \n",
    "    with open(savePath, 'w', newline = '', encoding = 'utf-8-sig') as csvfile:\n",
    "        csvWriter = csv.DictWriter(csvfile, fieldnames = fieldNames)\n",
    "        csvWriter.writeheader()\n",
    "\n",
    "for page in tqdm(range(startPage, endPage + 1)):\n",
    "    url = shamFind_url + '&page=' + str(page)\n",
    "    \n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(np.random.rand(1)[0])\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    re_list = soup.find_all('div', {'class' : 'bbs_list_box flex_spbtw'})\n",
    "\n",
    "    for res in re_list:\n",
    "        href = res.find('h3', {'class' : 'cont_subject'}).find('a')['href']\n",
    "        view = res.find('div', {'class' : 'cont_info_hit'}).text\n",
    "\n",
    "        driver.get(href)\n",
    "        driver.implicitly_wait(np.random.rand(1)[0])\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        content = soup.find_all('p', {'class' : 'cont'})\n",
    "        howToUse = content[0].text\n",
    "        ingredients = content[1].text\n",
    "        try:\n",
    "            image = soup.find('img', {'class' : 'lazy'})['src']\n",
    "        except:\n",
    "            image = np.nan\n",
    "            \n",
    "        tag = soup.find('div', {'class' : 'tag'}).text\n",
    "        title = soup.find('div', {'class' : 'subject'}).text\n",
    "        brand = soup.find('div', {'class' : 'brand'}).text\n",
    "        company = soup.find('div', {'class' : 'company'}).text\n",
    "        reviewNum = int(''.join(numCompile.findall(soup.find('div', {'class' : 'score_title'}).text)))\n",
    "        \n",
    "        maxPageNum = 0\n",
    "        if tag == '탈모의료기기':\n",
    "            volume = np.nan\n",
    "            price = soup.find('span', {'class' : 'price'}).text\n",
    "        else:\n",
    "            try:\n",
    "                volumeAndPrice = soup.find('div', {'class' : 'volume_price'}).text.strip().split('\\n')\n",
    "                volume = volumeAndPrice[0]\n",
    "                price = volumeAndPrice[1]\n",
    "            except:\n",
    "                volume = np.nan\n",
    "                price = np.nan\n",
    "        \n",
    "        result1 = [href, title, reviewNum, tag, brand, company, howToUse, ingredients, image, volume, price]\n",
    "        \n",
    "        if reviewNum == 0:\n",
    "            result2 = []\n",
    "            saveResult(result1, result2)\n",
    "            continue\n",
    "            \n",
    "        elif reviewNum <= 20:\n",
    "            result2 = crawReview(soup)\n",
    "            saveResult(result1, result2)\n",
    "            continue\n",
    "        else:\n",
    "            pageNextExist = True\n",
    "            nextButtonClicked = False\n",
    "            \n",
    "            while pageNextExist:\n",
    "                pageNext = soup.find('a', {'class' : 'pg_page pg_next'})\n",
    "                if pd.isnull(pageNext):\n",
    "                    pageNextExist = False\n",
    "                result2 = crawReview(soup)\n",
    "                saveResult(result1, result2)\n",
    "                \n",
    "                pgPageList = driver.find_elements(By.CLASS_NAME, 'pg_page')\n",
    "                pgPageList_soup = soup.find_all('a', {'class' : 'pg_page'})\n",
    "                pgPageList_soup = [int(pg['data-page_no']) for pg in pgPageList_soup]\n",
    "                \n",
    "                for i in range(len(pgPageList)):\n",
    "                    if nextButtonClicked & (i == 0):\n",
    "                        continue\n",
    "                    \n",
    "                    if pgPageList_soup[i] > maxPageNum:\n",
    "                        maxPageNum = pgPageList_soup[i]                    \n",
    "                    else :\n",
    "                        pageNextExist = False\n",
    "                        break\n",
    "                    \n",
    "                    pgPageList[i].click()\n",
    "                    pgPageList = driver.find_elements(By.CLASS_NAME, 'pg_page')\n",
    "                    driver.implicitly_wait(np.random.rand(1)[0])\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    result2 = crawReview(soup)\n",
    "                    saveResult(result1, result2)\n",
    "                    \n",
    "                    pageNext = soup.find('a', {'class' : 'pg_page pg_next'})\n",
    "                    if pd.isnull(pageNext):\n",
    "                        pageNextExist = False\n",
    "                nextButtonClicked = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da0fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b13513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigDataUtilization2023",
   "language": "python",
   "name": "bigdatautilization2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
