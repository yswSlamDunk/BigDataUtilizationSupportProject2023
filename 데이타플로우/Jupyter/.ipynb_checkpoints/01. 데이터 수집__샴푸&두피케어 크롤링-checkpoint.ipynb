{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1fd481c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'webdriver_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14125/3820369078.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwebdriver_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchrome\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChromeDriverManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchrome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mService\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mChromeService\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'webdriver_manager'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "import multiprocessing\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6372f06b",
   "metadata": {},
   "source": [
    "# 샴푸&두피케어_샴푸&두피제품 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5c88136",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_url = 'https://daedamo.com/new/'\n",
    "login_url = 'https://daedamo.com/new/bbs/login.php'\n",
    "\n",
    "shamFind_url = 'https://daedamo.com/ingre?sca=%ED%83%88%EB%AA%A8%EA%B4%80%EB%A0%A8%EC%83%81%ED%92%88&bso=A&overlaps=5'\n",
    "startPage = 1\n",
    "endPage = 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d36d3766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawReview(soup):\n",
    "    commentList = soup.find_all('div', {'class', 'comment-item'})\n",
    "    \n",
    "    totalScoreList = []\n",
    "    satisfactionScoreList = []\n",
    "    priceScoreList = []\n",
    "    rebuyScoreList = []\n",
    "    commenterList = []\n",
    "    commentDateList = []\n",
    "    commentGoodList = []\n",
    "    commentBadList = []\n",
    "    commentContentList = []\n",
    "    \n",
    "    for comment in commentList:\n",
    "        reviewScoreList = comment.find_all('div', {'class', 'review-item'})\n",
    "\n",
    "        commenter = comment.find('span', {'class' : 'sv_wrap'})\n",
    "        if pd.isnull(commenter):\n",
    "            commenter = comment.find('span', {'class' : 'guest'}).text\n",
    "        else:\n",
    "            commenter = comment.find('a')['title'].replace(' 자기소개', '')\n",
    "        \n",
    "        commentDate = comment.find('span', {'class' : 'datetime'}).text  \n",
    "        commentContent = comment.find('div', {'class' : 'contents'})\n",
    "        \n",
    "        \n",
    "        commenterList.append(commenter)\n",
    "        commentDateList.append(commentDate)\n",
    "        \n",
    "        \n",
    "        if len(reviewScoreList) == 1:\n",
    "            satisfactionScore = np.nan\n",
    "            priceScore = np.nan\n",
    "            rebuyScore = np.nan\n",
    "            totalScore = reviewScoreList[0].find('span')['style'].replace('width:', '')[:-1]\n",
    "            good = np.nan\n",
    "            bad = np.nan\n",
    "            commentContent = commentContent.text\n",
    "                \n",
    "        else:\n",
    "            satisfactionScore = reviewScoreList[0].find('span')['style'].replace('width:', '')[:-1]\n",
    "            priceScore = reviewScoreList[1].find('span')['style'].replace('width:', '')[:-1]\n",
    "            rebuyScore = reviewScoreList[2].find('span')['style'].replace('width:', '')[:-1]\n",
    "            totalScore = np.nan\n",
    "            \n",
    "            tmp_content = commentContent.find_all('dt')\n",
    "            \n",
    "            if len(tmp_content) == 2:\n",
    "                commentContent = commentContent.text.split('아쉬운점')\n",
    "                good = commentContent[0].split('좋은점')[1]\n",
    "                bad = commentContent[1]\n",
    "                \n",
    "            else:\n",
    "                good = commentContent.text.split('좋은점')[1]\n",
    "                bad = np.nan\n",
    "            commentContent = np.nan\n",
    "            \n",
    "        commentGoodList.append(good)\n",
    "        commentBadList.append(bad)\n",
    "        commentContentList.append(commentContent)      \n",
    "        \n",
    "        satisfactionScoreList.append(satisfactionScore)\n",
    "        priceScoreList.append(priceScore)\n",
    "        rebuyScoreList.append(rebuyScore)\n",
    "        totalScoreList.append(totalScore)\n",
    "\n",
    "    return [totalScoreList, satisfactionScoreList, priceScoreList, rebuyScoreList, commenterList, commentDateList, commentContentList, commentGoodList, commentBadList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd538a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResult(result1, resut2, savePath = '../Data/샴푸&두피제품 찾기.csv', fieldNames = ['href', 'title', 'reviewNum', 'tag', 'brand', 'company', 'howToUse', 'ingredients', 'image', 'volume', 'price', 'totalScore', 'satisfactionScore', 'priceScore', 'rebuyScore', 'commenter', 'commentDate', 'commentContent', 'commentGood', 'commentBad']):\n",
    "        if result2 == []:\n",
    "            with open(savePath, 'a', newline = '', encoding = 'utf-8-sig') as csvfile:\n",
    "                csvWriter = csv.DictWriter(csvfile, fieldnames = fieldNames)\n",
    "                csvWriter.writerow({'href' : result1[0],\n",
    "                                    'title' : result1[1],\n",
    "                                    'reviewNum' : result1[2],\n",
    "                                    'tag' : result1[3],\n",
    "                                    'brand' : result1[4],\n",
    "                                    'company' : result1[5],\n",
    "                                    'howToUse' : result1[6],\n",
    "                                    'ingredients' : result1[7],\n",
    "                                    'image' : result1[8],\n",
    "                                    'volume' : result1[9],\n",
    "                                    'price' : result1[10],\n",
    "                                    'totalScore' : np.nan,\n",
    "                                    'satisfactionScore' : np.nan,\n",
    "                                    'priceScore' : np.nan,\n",
    "                                    'rebuyScore' : np.nan,\n",
    "                                    'commenter' : np.nan,\n",
    "                                    'commentDate' : np.nan,\n",
    "                                    'commentContent' : np.nan,\n",
    "                                    'commentGood' : np.nan,\n",
    "                                    'commentBad' : np.nan\n",
    "                                   })\n",
    "        else:\n",
    "            tmp = [np.nan]*(len(result2[0]) -1)\n",
    "            href = [result1[0]] + tmp\n",
    "            title = [result1[1]] + tmp\n",
    "            reviewNum = [result1[2]] + tmp\n",
    "            tag = [result1[3]] + tmp\n",
    "            brand = [result1[4]] + tmp\n",
    "            company = [result1[5]] + tmp\n",
    "            howToUse = [result1[6]] + tmp\n",
    "            ingredients = [result1[7]] + tmp\n",
    "            image = [result1[8]] + tmp\n",
    "            volume = [result1[9]] + tmp\n",
    "            price = [result1[10]] + tmp\n",
    "            pd.DataFrame({'href' : href,\n",
    "                          'title' : title,\n",
    "                          'reviewNum' : reviewNum,\n",
    "                          'tag' : tag,\n",
    "                          'brand' : brand,\n",
    "                          'company' : company,\n",
    "                          'howToUse' : howToUse,\n",
    "                          'ingredients' : ingredients,\n",
    "                          'image' : image,\n",
    "                          'volume' : volume,\n",
    "                          'price' : price,\n",
    "                          'totalScore' : result2[0],\n",
    "                          'satisfactionScore' : result2[1],\n",
    "                          'priceScore' : result2[2],\n",
    "                          'rebuyScore' : result2[3],\n",
    "                          'commenter' : result2[4],\n",
    "                          'commentDate' : result2[5],\n",
    "                          'commentContent' : result2[6],\n",
    "                          'commentGood' : result2[7],\n",
    "                          'commentBad' : result2[8],\n",
    "                         }).to_csv(savePath, mode = 'a', index = False, header = None)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87273d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82613ad0dbd477ab53b191c4396d01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disabled-dev-shm-usage')\n",
    "chrome_options.add_argument('--blink-settings=imagesEnabled=false')\n",
    "chrome_options.add_argument('Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36')\n",
    "\n",
    "savePath = '../Data/샴푸&두피제품 찾기.csv'\n",
    "fieldNames = ['href', 'title', 'reviewNum', 'tag', 'brand', 'company', 'howToUse', 'ingredients', \\\n",
    "             'image', 'volume', 'price', 'totalScore', 'satisfactionScore', 'priceScore', \\\n",
    "             'rebuyScore', 'commenter', 'commentDate', 'commentContent', 'commentGood', 'commentBad']\n",
    "\n",
    "driver = webdriver.Chrome(options = chrome_options)\n",
    "\n",
    "driver.get(login_url)\n",
    "driver.implicitly_wait(np.random.rand(1)[0])\n",
    "\n",
    "driver.find_element_by_id('login_id').send_keys('dataflow28')\n",
    "driver.find_element_by_id('login_pw').send_keys('flowdata2833!!')\n",
    "driver.find_element_by_class_name('btn_submit').click()\n",
    "driver.implicitly_wait(np.random.rand(1)[0])\n",
    "time.sleep(2)\n",
    "\n",
    "numCompile = re.compile('\\d')\n",
    "\n",
    "if not os.path.isfile(savePath):\n",
    "    tmp = pd.DataFrame(columns = fieldNames)\n",
    "    tmp.to_csv(savePath, encoding = 'utf-8-sig')\n",
    "    \n",
    "    with open(savePath, 'w', newline = '', encoding = 'utf-8-sig') as csvfile:\n",
    "        csvWriter = csv.DictWriter(csvfile, fieldnames = fieldNames)\n",
    "        csvWriter.writeheader()\n",
    "\n",
    "for page in tqdm(range(startPage, endPage + 1)):\n",
    "    url = shamFind_url + '&page=' + str(page)\n",
    "    \n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(np.random.rand(1)[0])\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    re_list = soup.find_all('div', {'class' : 'bbs_list_box flex_spbtw'})\n",
    "\n",
    "    for res in re_list:\n",
    "        href = res.find('h3', {'class' : 'cont_subject'}).find('a')['href']\n",
    "        view = res.find('div', {'class' : 'cont_info_hit'}).text\n",
    "\n",
    "        driver.get(href)\n",
    "        driver.implicitly_wait(np.random.rand(1)[0])\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        content = soup.find_all('p', {'class' : 'cont'})\n",
    "        howToUse = content[0].text\n",
    "        ingredients = content[1].text\n",
    "        try:\n",
    "            image = soup.find('img', {'class' : 'lazy'})['src']\n",
    "        except:\n",
    "            image = np.nan\n",
    "            \n",
    "        tag = soup.find('div', {'class' : 'tag'}).text\n",
    "        title = soup.find('div', {'class' : 'subject'}).text\n",
    "        brand = soup.find('div', {'class' : 'brand'}).text\n",
    "        company = soup.find('div', {'class' : 'company'}).text\n",
    "        reviewNum = int(''.join(numCompile.findall(soup.find('div', {'class' : 'score_title'}).text)))\n",
    "        \n",
    "        maxPageNum = 0\n",
    "        if tag == '탈모의료기기':\n",
    "            volume = np.nan\n",
    "            price = soup.find('span', {'class' : 'price'}).text\n",
    "        else:\n",
    "            try:\n",
    "                volumeAndPrice = soup.find('div', {'class' : 'volume_price'}).text.strip().split('\\n')\n",
    "                volume = volumeAndPrice[0]\n",
    "                price = volumeAndPrice[1]\n",
    "            except:\n",
    "                volume = np.nan\n",
    "                price = np.nan\n",
    "        \n",
    "        result1 = [href, title, reviewNum, tag, brand, company, howToUse, ingredients, image, volume, price]\n",
    "        \n",
    "        if reviewNum == 0:\n",
    "            result2 = []\n",
    "            saveResult(result1, result2)\n",
    "            continue\n",
    "            \n",
    "        elif reviewNum <= 20:\n",
    "            result2 = crawReview(soup)\n",
    "            saveResult(result1, result2)\n",
    "            continue\n",
    "        else:\n",
    "            pageNextExist = True\n",
    "            nextButtonClicked = False\n",
    "            \n",
    "            while pageNextExist:\n",
    "                pageNext = soup.find('a', {'class' : 'pg_page pg_next'})\n",
    "                if pd.isnull(pageNext):\n",
    "                    pageNextExist = False\n",
    "                result2 = crawReview(soup)\n",
    "                saveResult(result1, result2)\n",
    "                \n",
    "                pgPageList = driver.find_elements(By.CLASS_NAME, 'pg_page')\n",
    "                pgPageList_soup = soup.find_all('a', {'class' : 'pg_page'})\n",
    "                pgPageList_soup = [int(pg['data-page_no']) for pg in pgPageList_soup]\n",
    "                \n",
    "                for i in range(len(pgPageList)):\n",
    "                    if nextButtonClicked & (i == 0):\n",
    "                        continue\n",
    "                    \n",
    "                    if pgPageList_soup[i] > maxPageNum:\n",
    "                        maxPageNum = pgPageList_soup[i]                    \n",
    "                    else :\n",
    "                        pageNextExist = False\n",
    "                        break\n",
    "                    \n",
    "                    pgPageList[i].click()\n",
    "                    pgPageList = driver.find_elements(By.CLASS_NAME, 'pg_page')\n",
    "                    driver.implicitly_wait(np.random.rand(1)[0])\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    html = driver.page_source\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    result2 = crawReview(soup)\n",
    "                    saveResult(result1, result2)\n",
    "                    \n",
    "                    pageNext = soup.find('a', {'class' : 'pg_page pg_next'})\n",
    "                    if pd.isnull(pageNext):\n",
    "                        pageNextExist = False\n",
    "                nextButtonClicked = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10648b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "218e4851",
   "metadata": {},
   "source": [
    "# 샴푸&두피케어_체험평가단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50b13513",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clawringHref(url, pageNum, chrome_options, startPage = 1):\n",
    "    if not os.path.isfile(save_path):\n",
    "        tmp = pd.DataFrame(columns = fieldNames)\n",
    "        tmp.to_csv(save_path, index = False, encoding = 'utf-8-sig', header=fieldNames)\n",
    "\n",
    "    for page in tqdm(range(startPage, pageNum + 1)):\n",
    "        page = str(page)\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                response = requests.get(url + page)\n",
    "                if response.status_code == 200:\n",
    "                    html = response.text\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    \n",
    "                    notice_list = soup.find_all('div', {'class' : ['notice_wrap']})\n",
    "                    re_list = soup.find_all('div', {'class' : ['list_wrap']})\n",
    "                    re_list = [str(re) for re in re_list]\n",
    "                    \n",
    "                    for re in re_list:\n",
    "                        parsing_save(re, chrome_options)\n",
    "                    break\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d18d32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsing_save(re, chrome_options):\n",
    "    re = BeautifulSoup(re, 'html.parser')\n",
    "    category = re.find('div', {'class' : 'mw_basic_list_category'}).text\n",
    "    date = re.find('div', {'class' : 'mw_basic_list_datetime media-no-text'}).text\n",
    "    view = re.find('div', {'class' : 'mw_basic_list_hit media-no-text'}).text\n",
    "    title = re.find('h3', {'class' : 'media-list-subject'}).text.replace('\\n', '')\n",
    "    rank = str(re.find('img')['src']).split('/')[-1].split('.')[0]\n",
    "    href = re.find_all('a')[2]['href']\n",
    "    \n",
    "    print(href)\n",
    "    \n",
    "    try:\n",
    "        writer = re.find('a', {'class' : 'sv_member'}).text.strip()\n",
    "    except:\n",
    "        writer = re.find('div', {'mw_basic_list_name media-no-text'}).text\n",
    "\n",
    "    content, imageList = clawringContentImage(href, chrome_options)\n",
    "\n",
    "    try:\n",
    "        pd.DataFrame({'title' : [title],\n",
    "                      'category' : [category],\n",
    "                      'date' : [date],\n",
    "                      'writer' : [writer],\n",
    "                      'rank' : [rank],\n",
    "                      'view' : [view],\n",
    "                      'href' : [href],\n",
    "                      'content' : [content.strip()],\n",
    "                      'imageList' : [imageList]}).to_csv(save_path,\n",
    "                                                         mode = 'a',\n",
    "                                                         index = False,\n",
    "                                                         header = False,\n",
    "                                                         encoding = 'utf-8-sig')\n",
    "    except:\n",
    "        print({'title' : title,\n",
    "                'category' : category,\n",
    "                'date' : date,\n",
    "                'writer' : writer,\n",
    "                'rank' : rank,\n",
    "                'view' : view,\n",
    "                'href' : href,\n",
    "                'content' : content,\n",
    "                'imageList' : imageList})\n",
    "        print(href, ': Error', '\\n')\n",
    "        print('==================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad19a946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clawringContentImage(href, chrome_options):\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "    driver.get(href)\n",
    "    driver.implicitly_wait(np.random.rand(1)[0])\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    res = soup.find('div', {'class' : 'mw_basic_view_content'})\n",
    "\n",
    "    try:\n",
    "        content = res.text\n",
    "    except:\n",
    "        return np.nan, []\n",
    "\n",
    "    images = res.find_all('img', {'class' : 'lazy content-image'})\n",
    "\n",
    "    if images != []:\n",
    "        imageList = [url['src'] for url in res.find_all('img', {'class' : 'lazy'})]\n",
    "    else:\n",
    "        imageList = []\n",
    "\n",
    "    return content, imageList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b336277",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument('--headless')\n",
    "chrome_options.add_experimental_option(\"detach\", True) \n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disabled-dev-shm-usage')\n",
    "chrome_options.add_argument('--blink-settings=imagesEnabled=false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ec3171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://daedamo.com/job/4616?overlaps=5\n",
      "https://daedamo.com/job/4611?overlaps=5\n",
      "https://daedamo.com/job/4609?overlaps=5\n",
      "https://daedamo.com/job/4608?overlaps=5\n",
      "https://daedamo.com/job/4606?overlaps=5\n",
      "https://daedamo.com/job/4605?overlaps=5\n",
      "https://daedamo.com/job/4599?overlaps=5\n",
      "https://daedamo.com/job/4597?overlaps=5\n",
      "https://daedamo.com/job/4596?overlaps=5\n",
      "https://daedamo.com/job/4595?overlaps=5\n",
      "https://daedamo.com/job/4592?overlaps=5\n",
      "https://daedamo.com/job/4591?overlaps=5\n",
      "https://daedamo.com/job/4590?overlaps=5\n",
      "https://daedamo.com/job/4589?overlaps=5\n",
      "https://daedamo.com/job/4588?overlaps=5\n",
      "https://daedamo.com/job/4587?overlaps=5\n",
      "https://daedamo.com/job/4586?overlaps=5\n",
      "https://daedamo.com/job/4585?overlaps=5\n",
      "https://daedamo.com/job/4584?overlaps=5\n",
      "https://daedamo.com/job/4583?overlaps=5\n",
      "https://daedamo.com/job/4582?overlaps=5\n",
      "https://daedamo.com/job/4581?overlaps=5\n",
      "https://daedamo.com/job/4580?overlaps=5\n",
      "https://daedamo.com/job/4579?overlaps=5\n",
      "https://daedamo.com/job/4578?overlaps=5\n",
      "https://daedamo.com/job/4577?overlaps=5\n",
      "https://daedamo.com/job/4576?overlaps=5\n",
      "https://daedamo.com/job/4575?overlaps=5\n",
      "https://daedamo.com/job/4574?overlaps=5\n",
      "https://daedamo.com/job/4573?overlaps=5\n",
      "https://daedamo.com/job/4571?overlaps=5\n",
      "https://daedamo.com/job/4570?overlaps=5\n",
      "https://daedamo.com/job/4569?overlaps=5\n",
      "https://daedamo.com/job/4568?overlaps=5\n",
      "https://daedamo.com/job/4567?overlaps=5\n",
      "https://daedamo.com/job/4566?overlaps=5\n",
      "https://daedamo.com/job/4565?overlaps=5\n",
      "https://daedamo.com/job/4564?overlaps=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▉                                    | 1/39 [06:00<3:48:26, 360.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://daedamo.com/job/4563?overlaps=5\n",
      "https://daedamo.com/job/4562?overlaps=5\n",
      "https://daedamo.com/job/4561?overlaps=5\n",
      "https://daedamo.com/job/4560?overlaps=5\n",
      "https://daedamo.com/job/4559?overlaps=5\n",
      "https://daedamo.com/job/4558?overlaps=5\n",
      "https://daedamo.com/job/4556?overlaps=5\n",
      "https://daedamo.com/job/4552?overlaps=5\n",
      "https://daedamo.com/job/4551?overlaps=5\n",
      "https://daedamo.com/job/4549?overlaps=5\n",
      "https://daedamo.com/job/4547?overlaps=5\n",
      "https://daedamo.com/job/4541?overlaps=5\n",
      "https://daedamo.com/job/4539?overlaps=5\n",
      "https://daedamo.com/job/4537?overlaps=5\n",
      "https://daedamo.com/job/4536?overlaps=5\n",
      "https://daedamo.com/job/4533?overlaps=5\n",
      "https://daedamo.com/job/4532?overlaps=5\n",
      "https://daedamo.com/job/4529?overlaps=5\n",
      "https://daedamo.com/job/4528?overlaps=5\n",
      "https://daedamo.com/job/4527?overlaps=5\n",
      "https://daedamo.com/job/4525?overlaps=5\n",
      "https://daedamo.com/job/4518?overlaps=5\n",
      "https://daedamo.com/job/4517?overlaps=5\n",
      "https://daedamo.com/job/4516?overlaps=5\n",
      "https://daedamo.com/job/4511?overlaps=5\n",
      "https://daedamo.com/job/4510?overlaps=5\n",
      "https://daedamo.com/job/4509?overlaps=5\n",
      "https://daedamo.com/job/4507?overlaps=5\n",
      "https://daedamo.com/job/4506?overlaps=5\n",
      "https://daedamo.com/job/4505?overlaps=5\n",
      "https://daedamo.com/job/4504?overlaps=5\n",
      "https://daedamo.com/job/4503?overlaps=5\n",
      "https://daedamo.com/job/4502?overlaps=5\n",
      "https://daedamo.com/job/4501?overlaps=5\n",
      "https://daedamo.com/job/4500?overlaps=5\n",
      "https://daedamo.com/job/4499?overlaps=5\n",
      "https://daedamo.com/job/4498?overlaps=5\n",
      "https://daedamo.com/job/4497?overlaps=5\n",
      "https://daedamo.com/job/4495?overlaps=5\n",
      "https://daedamo.com/job/4494?overlaps=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|█▉                                   | 2/39 [12:18<3:48:40, 370.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://daedamo.com/job/4493?overlaps=5\n",
      "https://daedamo.com/job/4491?overlaps=5\n",
      "https://daedamo.com/job/4490?overlaps=5\n",
      "https://daedamo.com/job/4488?overlaps=5\n",
      "https://daedamo.com/job/4485?overlaps=5\n",
      "https://daedamo.com/job/4484?overlaps=5\n",
      "https://daedamo.com/job/4483?overlaps=5\n",
      "https://daedamo.com/job/4482?overlaps=5\n",
      "https://daedamo.com/job/4480?overlaps=5\n",
      "https://daedamo.com/job/4479?overlaps=5\n",
      "https://daedamo.com/job/4478?overlaps=5\n",
      "https://daedamo.com/job/4477?overlaps=5\n",
      "https://daedamo.com/job/4476?overlaps=5\n",
      "https://daedamo.com/job/4475?overlaps=5\n",
      "https://daedamo.com/job/4471?overlaps=5\n",
      "https://daedamo.com/job/4469?overlaps=5\n",
      "https://daedamo.com/job/4468?overlaps=5\n",
      "https://daedamo.com/job/4464?overlaps=5\n",
      "https://daedamo.com/job/4462?overlaps=5\n",
      "https://daedamo.com/job/4461?overlaps=5\n",
      "https://daedamo.com/job/4460?overlaps=5\n",
      "https://daedamo.com/job/4451?overlaps=5\n",
      "https://daedamo.com/job/4450?overlaps=5\n",
      "https://daedamo.com/job/4449?overlaps=5\n",
      "https://daedamo.com/job/4448?overlaps=5\n",
      "https://daedamo.com/job/4447?overlaps=5\n",
      "https://daedamo.com/job/4439?overlaps=5\n",
      "https://daedamo.com/job/4438?overlaps=5\n",
      "https://daedamo.com/job/4437?overlaps=5\n",
      "https://daedamo.com/job/4436?overlaps=5\n",
      "https://daedamo.com/job/4434?overlaps=5\n",
      "https://daedamo.com/job/4432?overlaps=5\n",
      "https://daedamo.com/job/4431?overlaps=5\n",
      "https://daedamo.com/job/4430?overlaps=5\n",
      "https://daedamo.com/job/4429?overlaps=5\n",
      "https://daedamo.com/job/4428?overlaps=5\n",
      "https://daedamo.com/job/4427?overlaps=5\n",
      "https://daedamo.com/job/4426?overlaps=5\n",
      "https://daedamo.com/job/4425?overlaps=5\n",
      "https://daedamo.com/job/4424?overlaps=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|██▊                                  | 3/39 [19:13<3:54:34, 390.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://daedamo.com/job/4423?overlaps=5\n",
      "https://daedamo.com/job/4422?overlaps=5\n",
      "https://daedamo.com/job/4417?overlaps=5\n",
      "https://daedamo.com/job/4416?overlaps=5\n",
      "https://daedamo.com/job/4415?overlaps=5\n",
      "https://daedamo.com/job/4414?overlaps=5\n",
      "https://daedamo.com/job/4413?overlaps=5\n",
      "https://daedamo.com/job/4412?overlaps=5\n",
      "https://daedamo.com/job/4411?overlaps=5\n",
      "https://daedamo.com/job/4410?overlaps=5\n",
      "https://daedamo.com/job/4409?overlaps=5\n",
      "https://daedamo.com/job/4408?overlaps=5\n",
      "https://daedamo.com/job/4407?overlaps=5\n",
      "https://daedamo.com/job/4406?overlaps=5\n",
      "https://daedamo.com/job/4405?overlaps=5\n",
      "https://daedamo.com/job/4403?overlaps=5\n",
      "https://daedamo.com/job/4402?overlaps=5\n",
      "https://daedamo.com/job/4401?overlaps=5\n",
      "https://daedamo.com/job/4400?overlaps=5\n",
      "https://daedamo.com/job/4399?overlaps=5\n",
      "https://daedamo.com/job/4398?overlaps=5\n",
      "https://daedamo.com/job/4397?overlaps=5\n",
      "https://daedamo.com/job/4396?overlaps=5\n",
      "https://daedamo.com/job/4395?overlaps=5\n",
      "https://daedamo.com/job/4394?overlaps=5\n",
      "https://daedamo.com/job/4393?overlaps=5\n",
      "https://daedamo.com/job/4392?overlaps=5\n",
      "https://daedamo.com/job/4391?overlaps=5\n",
      "https://daedamo.com/job/4390?overlaps=5\n",
      "https://daedamo.com/job/4389?overlaps=5\n",
      "https://daedamo.com/job/4388?overlaps=5\n",
      "https://daedamo.com/job/4386?overlaps=5\n",
      "https://daedamo.com/job/4384?overlaps=5\n",
      "https://daedamo.com/job/4383?overlaps=5\n",
      "https://daedamo.com/job/4382?overlaps=5\n",
      "https://daedamo.com/job/4381?overlaps=5\n",
      "https://daedamo.com/job/4380?overlaps=5\n",
      "https://daedamo.com/job/4379?overlaps=5\n",
      "https://daedamo.com/job/4378?overlaps=5\n",
      "https://daedamo.com/job/4377?overlaps=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|███▊                                 | 4/39 [25:50<3:49:25, 393.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://daedamo.com/job/4376?overlaps=5\n",
      "https://daedamo.com/job/4375?overlaps=5\n",
      "https://daedamo.com/job/4374?overlaps=5\n",
      "https://daedamo.com/job/4373?overlaps=5\n",
      "https://daedamo.com/job/4372?overlaps=5\n",
      "https://daedamo.com/job/4371?overlaps=5\n",
      "https://daedamo.com/job/4370?overlaps=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███▊                                 | 4/39 [26:54<3:55:24, 403.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://daedamo.com/job?overlaps=5&page=\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m pageNum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m39\u001b[39m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mclawringHref\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpageNum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchrome_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 20\u001b[0m, in \u001b[0;36mclawringHref\u001b[1;34m(url, pageNum, chrome_options, startPage)\u001b[0m\n\u001b[0;32m     17\u001b[0m         re_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(re) \u001b[38;5;28;01mfor\u001b[39;00m re \u001b[38;5;129;01min\u001b[39;00m re_list]\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m re \u001b[38;5;129;01min\u001b[39;00m re_list:\n\u001b[1;32m---> 20\u001b[0m             \u001b[43mparsing_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchrome_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError:\n",
      "Cell \u001b[1;32mIn[46], line 17\u001b[0m, in \u001b[0;36mparsing_save\u001b[1;34m(re, chrome_options)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     writer \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmw_basic_list_name media-no-text\u001b[39m\u001b[38;5;124m'\u001b[39m})\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m---> 17\u001b[0m content, imageList \u001b[38;5;241m=\u001b[39m \u001b[43mclawringContentImage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchrome_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m : [title],\n\u001b[0;32m     21\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m : [category],\n\u001b[0;32m     22\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m : [date],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m                                                      header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m                                                      encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[47], line 2\u001b[0m, in \u001b[0;36mclawringContentImage\u001b[1;34m(href, chrome_options)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclawringContentImage\u001b[39m(href, chrome_options):\n\u001b[1;32m----> 2\u001b[0m     driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mChromeService(\u001b[43mChromeDriverManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m      3\u001b[0m     driver\u001b[38;5;241m.\u001b[39mget(href)\n\u001b[0;32m      4\u001b[0m     driver\u001b[38;5;241m.\u001b[39mimplicitly_wait(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BigDataUtilization2023\\Lib\\site-packages\\webdriver_manager\\chrome.py:40\u001b[0m, in \u001b[0;36mChromeDriverManager.install\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minstall\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 40\u001b[0m     driver_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_driver_binary_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     os\u001b[38;5;241m.\u001b[39mchmod(driver_path, \u001b[38;5;241m0o755\u001b[39m)\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver_path\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BigDataUtilization2023\\Lib\\site-packages\\webdriver_manager\\core\\manager.py:35\u001b[0m, in \u001b[0;36mDriverManager._get_driver_binary_path\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_driver_binary_path\u001b[39m(\u001b[38;5;28mself\u001b[39m, driver):\n\u001b[1;32m---> 35\u001b[0m     binary_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary_path:\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m binary_path\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BigDataUtilization2023\\Lib\\site-packages\\webdriver_manager\\core\\driver_cache.py:105\u001b[0m, in \u001b[0;36mDriverCacheManager.find_driver\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m browser_version:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m driver_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cache_key_driver_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_metadata_content()\n\u001b[0;32m    108\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_metadata_key(driver)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BigDataUtilization2023\\Lib\\site-packages\\webdriver_manager\\core\\driver_cache.py:151\u001b[0m, in \u001b[0;36mDriverCacheManager.get_cache_key_driver_version\u001b[1;34m(self, driver)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_key_driver_version:\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache_key_driver_version\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_driver_version_to_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BigDataUtilization2023\\Lib\\site-packages\\webdriver_manager\\core\\driver.py:48\u001b[0m, in \u001b[0;36mDriver.get_driver_version_to_download\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver_version_to_download:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver_version_to_download\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_latest_release_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BigDataUtilization2023\\Lib\\site-packages\\webdriver_manager\\drivers\\chrome.py:54\u001b[0m, in \u001b[0;36mChromeDriver.get_latest_release_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_latest_release_version\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 54\u001b[0m     determined_browser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_browser_version_from_os\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     log(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet LATEST \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m version for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_browser_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m determined_browser_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(determined_browser_version) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m113\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BigDataUtilization2023\\Lib\\site-packages\\webdriver_manager\\core\\driver.py:63\u001b[0m, in \u001b[0;36mDriver.get_browser_version_from_os\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mUse-cases:\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m- for key in metadata;\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03mNote: the fallback may have collisions in user cases when previous browser was not uninstalled properly.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_browser_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_browser_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_os_system_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_browser_version_from_os\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_browser_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_browser_version\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BigDataUtilization2023\\Lib\\site-packages\\webdriver_manager\\core\\os_manager.py:159\u001b[0m, in \u001b[0;36mOperationSystemManager.get_browser_version_from_os\u001b[1;34m(self, browser_type)\u001b[0m\n\u001b[0;32m    157\u001b[0m     cmd_mapping \u001b[38;5;241m=\u001b[39m cmd_mapping[browser_type][OperationSystemManager\u001b[38;5;241m.\u001b[39mget_os_name()]\n\u001b[0;32m    158\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m PATTERN[browser_type]\n\u001b[1;32m--> 159\u001b[0m     version \u001b[38;5;241m=\u001b[39m \u001b[43mread_version_from_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m version\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BigDataUtilization2023\\Lib\\site-packages\\webdriver_manager\\core\\utils.py:45\u001b[0m, in \u001b[0;36mread_version_from_cmd\u001b[1;34m(cmd, pattern)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_version_from_cmd\u001b[39m(cmd, pattern):\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mPopen(\n\u001b[0;32m     40\u001b[0m             cmd,\n\u001b[0;32m     41\u001b[0m             stdout\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mPIPE,\n\u001b[0;32m     42\u001b[0m             stdin\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mDEVNULL,\n\u001b[0;32m     43\u001b[0m             shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     44\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m stream:\n\u001b[1;32m---> 45\u001b[0m         stdout \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[0;32m     46\u001b[0m         version \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(pattern, stdout)\n\u001b[0;32m     47\u001b[0m         version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\BigDataUtilization2023\\Lib\\subprocess.py:1196\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stdin_write(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout:\n\u001b[1;32m-> 1196\u001b[0m     stdout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path_save_base = '../Data'\n",
    "fieldNames = ['title', 'category', 'date', 'writer', 'rank', 'view', 'href', 'content', 'imageList']\n",
    "\n",
    "path_save = '샴푸&두피케어_체험평가단.csv'\n",
    "save_path = os.path.join(path_save_base, path_save)\n",
    "url = 'https://daedamo.com/job?overlaps=5&page='\n",
    "pageNum = 39\n",
    "clawringHref(url, pageNum, chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6629a4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e9f7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BigDataUtilization2023",
   "language": "python",
   "name": "bigdatautilization2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
